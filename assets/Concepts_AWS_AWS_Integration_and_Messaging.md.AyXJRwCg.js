import{f as e,h as t,j as n,t as r}from"./chunks/plugin-vue_export-helper.CMCX8Lry.js";const i=JSON.parse(`{"title":"AWS Integration and Messaging","description":"AWS Integration and Messaging","frontmatter":{"title":"AWS Integration and Messaging","description":"AWS Integration and Messaging","prev":{"text":"AWS","link":"./AWS"}},"headers":[],"relativePath":"Concepts/AWS/AWS_Integration_and_Messaging.md","filePath":"Concepts/AWS/AWS_Integration_and_Messaging.md","lastUpdated":1767382134000}`);var a={name:`Concepts/AWS/AWS_Integration_and_Messaging.md`};function o(r,i,a,o,s,c){return n(),e(`div`,null,[...i[0]||=[t(`<h1 id="aws-integration-and-messaging" tabindex="-1">AWS Integration and Messaging <a class="header-anchor" href="#aws-integration-and-messaging" aria-label="Permalink to “AWS Integration and Messaging”">​</a></h1><p>SQS, SNS and Kinesis</p><ul><li><p>When we start deploying multiple applications, they will inevitably need to communicate with one another</p></li><li><p>There are two patterns of application communication:</p><ol><li>Synchronous communications (application to application)</li><li>Asynchronous/Event based (application to queue to application)</li></ol></li><li><p>Synchronous between applications can be problematic if there are sudden spikes of traffic</p></li><li><p>What if you need to suddenly encode 1000 videos but usually it&#39;s 10?</p></li><li><p>In that case, it&#39;s better to decouple your applications,</p><ul><li>using SQS: queue model</li><li>using SNS: pub/sub model</li><li>using Kinesis: real-time streaming model</li></ul></li><li><p>These services can scale independently from our application!</p></li></ul><h2 id="amazon-sqs-simple-queue-service" tabindex="-1">Amazon SQS (Simple Queue Service) <a class="header-anchor" href="#amazon-sqs-simple-queue-service" aria-label="Permalink to “Amazon SQS (Simple Queue Service)”">​</a></h2><h3 id="amazon-sqs-standard-queue" tabindex="-1">Amazon SQS - Standard Queue <a class="header-anchor" href="#amazon-sqs-standard-queue" aria-label="Permalink to “Amazon SQS - Standard Queue”">​</a></h3><ul><li>Oldest offering (over 10 years old)</li><li>Fully managed service, used to decouple applications</li><li>Attributes: <ul><li>Unlimited throughput, unlimited number of messages in queue</li><li>Default retention of messages: 4 days, maximum of 14 days</li><li>Low latency (less than 10 ms on publish and receive)</li><li>Limitation of 256KB per message sent</li></ul></li><li>Can have duplicate messages (at least once delivery, occasionally)</li><li>Can have out of order messages (best effort ordering)</li></ul><h3 id="sqs-producing-messages" tabindex="-1">SQS - Producing Messages <a class="header-anchor" href="#sqs-producing-messages" aria-label="Permalink to “SQS - Producing Messages”">​</a></h3><ul><li>Produced to SQS using the SDK (SendMessage API)</li><li>The message is persisted in SQS until a consumer deletes it</li><li>Message retention: default 4 days, up to 14 days</li><li>Example: send an order to be processed <ul><li>Order id</li><li>Customer id</li><li>Any attributes you want</li></ul></li><li>SQS standard: unlimited throughput</li></ul><h3 id="sqs-consuming-messages" tabindex="-1">SQS - Consuming Messages <a class="header-anchor" href="#sqs-consuming-messages" aria-label="Permalink to “SQS - Consuming Messages”">​</a></h3><ul><li>Consumers (running on EC2 instances, servers, or AWS Lambda)..</li><li>Poll SQS for messages (receive up to 10 messages at a time)</li><li>Process the messages (example: insert the message into an RDS database)</li><li>Delete the messages using the DeleteMessage API</li></ul><h3 id="sqs-multiple-ec2-instances-consumers" tabindex="-1">SQS - Multiple EC2 Instances Consumers <a class="header-anchor" href="#sqs-multiple-ec2-instances-consumers" aria-label="Permalink to “SQS - Multiple EC2 Instances Consumers”">​</a></h3><ul><li>Consumers receive and process messages in parallel</li><li>At least once delivery</li><li>Best-effort message ordering</li><li>Consumers delete messages after processing them</li><li>We can scale consumers horizontally to improve throughput of processing</li></ul><h3 id="amazon-sqs-security" tabindex="-1">Amazon SQS - Security <a class="header-anchor" href="#amazon-sqs-security" aria-label="Permalink to “Amazon SQS - Security”">​</a></h3><ul><li>Encryption: <ul><li>In-flight encryption using HTTPS API</li><li>At-rest encryption using KMS keys</li><li>Client-side encryption if the client wants to perform encryption/decryption itself</li></ul></li><li>Access Controls: IAM policies to regulate access to the SQS API</li><li>SQS Access Policies (similar to S3 bucket policies) <ul><li>Useful for cross-account access to SQS queues</li><li>Useful for allowing other services (SNS, S3...) to write to an SQS queue</li></ul></li></ul><h3 id="sqs-queue-access-policy" tabindex="-1">SQS Queue Access Policy <a class="header-anchor" href="#sqs-queue-access-policy" aria-label="Permalink to “SQS Queue Access Policy”">​</a></h3><p>Similar to S3 access policy</p><h3 id="sqs-message-visibility-timeout" tabindex="-1">SQS - Message Visibility Timeout <a class="header-anchor" href="#sqs-message-visibility-timeout" aria-label="Permalink to “SQS - Message Visibility Timeout”">​</a></h3><ul><li><p>After a message is polled by a consumer, it becomes invisible to other consumers</p></li><li><p>By default, the &quot;message visibility timeout&quot; is 30 seconds</p></li><li><p>That means the message has 30 seconds to be processed</p></li><li><p>After the message visibility timeout is over, the message is &quot;visible&quot; in SQS</p></li><li><p>If a message is not processed within the visibility timeout, it will be processed twice</p></li><li><p>A consumer could call the ChangeMessageVisibility API to get more time</p></li><li><p>If visibility timeout is high (hours), and consumer crashes, re-processing will take time</p></li><li><p>If visibility timeout is too low (seconds), we may get duplicates</p></li></ul><h3 id="amazon-sqs-dead-letter-queue" tabindex="-1">Amazon SQS - Dead Letter Queue <a class="header-anchor" href="#amazon-sqs-dead-letter-queue" aria-label="Permalink to “Amazon SQS - Dead Letter Queue”">​</a></h3><ul><li>If a consumer fails to process a message within the Visibility Timeout... the message goes back to the queue!</li><li>We can set a threshold of how many times a message can go back to the queue</li><li>After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ)</li><li>Useful for debugging!</li><li>Make sure to process the messages in the DLQ before they expire: <ul><li>Good to set a retention of 14 days in the DLQ</li></ul></li></ul><h3 id="amazon-sqs-delay-queue" tabindex="-1">Amazon SQS - Delay Queue <a class="header-anchor" href="#amazon-sqs-delay-queue" aria-label="Permalink to “Amazon SQS - Delay Queue”">​</a></h3><ul><li>Delay a message (consumers don&#39;t see it immediately) up to 15 minutes</li><li>Default is 0 seconds (message is available right away)</li><li>Can set a default at queue level</li><li>Can override the default on send using the DelaySeconds parameter</li></ul><h3 id="amazon-sqs-long-polling" tabindex="-1">Amazon SQS - Long Polling <a class="header-anchor" href="#amazon-sqs-long-polling" aria-label="Permalink to “Amazon SQS - Long Polling”">​</a></h3><ul><li>When a consumer requests messages from the queue, it can optionally &quot;wait&quot; for messages to arrive if there are none in the queue</li><li>This is called Long Polling</li><li>LongPolling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application</li><li>The wait time can be between 1 sec to 20 sec (20 sec preferable)</li><li>Long Polling is preferable to Short Polling</li><li>Long polling can be enabled at the queue level or at the API level using WaitTimeSeconds</li></ul><h3 id="sqs-extended-client" tabindex="-1">SQS Extended Client <a class="header-anchor" href="#sqs-extended-client" aria-label="Permalink to “SQS Extended Client”">​</a></h3><ul><li>Message size limit is 256KB, how to send large messages, e.g. 1GB?</li><li>Using the SQS Extended Client (Java Library)</li></ul><h3 id="sqs-must-know-api" tabindex="-1">SQS - Must know API <a class="header-anchor" href="#sqs-must-know-api" aria-label="Permalink to “SQS - Must know API”">​</a></h3><ul><li>CreateQueue (MessageRetentionPeriod), DeleteQueue</li><li>PurgeQueue: delete all the messages in queue</li><li>SendMessage (DelaySeconds), ReceiveMessage, DeleteMessage</li><li>MaxNumberOfMessages: default 1, max 10 (for ReceiveMessage API)</li><li>ReceiveMessageWaitTimeSeconds: Long Polling</li><li>ChangeMessageVisibility: change the message timeout</li><li>Batch APIs for SendMessage, DeleteMessage, ChangeMessageVisibility helps decrease your costs</li></ul><h3 id="amazon-sqs-fifo-queue" tabindex="-1">Amazon SQS - FIFO Queue <a class="header-anchor" href="#amazon-sqs-fifo-queue" aria-label="Permalink to “Amazon SQS - FIFO Queue”">​</a></h3><ul><li>FIFO = First In First Out (ordering of messages in the queue)</li><li>Limited throughput: 300 msg/s without batching, 3000 msg/s with</li><li>Exactly-once send capability (by removing duplicates)</li><li>Messages are processed in order by the consumer</li></ul><h3 id="sqs-fifo-deduplication" tabindex="-1">SQS FIFO - Deduplication <a class="header-anchor" href="#sqs-fifo-deduplication" aria-label="Permalink to “SQS FIFO - Deduplication”">​</a></h3><ul><li>De-duplication interval is 5 minutes</li><li>Two de-duplication methods: <ul><li>Content-based deduplication: will do a SHA-256 hash of the message body</li><li>Explicitly provide a Message Deduplication ID</li></ul></li></ul><h3 id="sqs-fifo-message-grouping" tabindex="-1">SQS FIFO - Message Grouping <a class="header-anchor" href="#sqs-fifo-message-grouping" aria-label="Permalink to “SQS FIFO - Message Grouping”">​</a></h3><ul><li>If you specify the same value of MessageGroupID in an SQS FIFO queue, you can only have one consumer, and all the messages are in order</li><li>To get ordering at the level of a subset of messages, specify different values for MessageGroupID <ul><li>Messages that share a common Message Group ID will be in order within the group</li><li>Each Group ID can have a different consumer (parallel processing!)</li><li>Ordering across groups is not guaranteed</li></ul></li></ul><h2 id="amazon-sns" tabindex="-1">Amazon SNS <a class="header-anchor" href="#amazon-sns" aria-label="Permalink to “Amazon SNS”">​</a></h2><ul><li><p>What if you want to send one message to many receivers?</p></li><li><p>The &quot;event producer&quot; only sends message to one SNS topic</p></li><li><p>As many &quot;event receivers&quot; (subscriptions) as we want to listen to the SNS topic notifications</p></li><li><p>Each subscriber to the topic will get all the messages (note: new feature to filter messages)</p></li><li><p>Up to 10,000,000 subscriptions per topic</p></li><li><p>100,000 topics limit</p></li><li><p>Subscribers can be:</p><ul><li>SQS</li><li>HTTP / HTTPS (with delivery retries - how many times)</li><li>Lambda</li><li>Emails</li><li>SMS messages</li><li>Mobile Notifications</li></ul></li></ul><h3 id="sns-integrates-with-a-lot-of-aws-services" tabindex="-1">SNS integrates with a lot of AWS services <a class="header-anchor" href="#sns-integrates-with-a-lot-of-aws-services" aria-label="Permalink to “SNS integrates with a lot of AWS services”">​</a></h3><ul><li>Many AWS services can send data directly to SNS for notifications</li><li>CloudWatch (for alarms)</li><li>Auto Scaling Groups notifications</li><li>Amazon S3 (on bucket events)</li><li>CloudFormation (upon state changes =&gt; failed to build, etc)</li><li>Etc...</li></ul><h3 id="amazon-sns-how-to-publish" tabindex="-1">Amazon SNS - How to publish <a class="header-anchor" href="#amazon-sns-how-to-publish" aria-label="Permalink to “Amazon SNS - How to publish”">​</a></h3><ul><li>Topic Publish (using the SDK) <ul><li>Create a topic</li><li>Create a subscription (or many)</li><li>Publish to the topic</li></ul></li><li>Direct Publish (for mobile apps SDK) <ul><li>Create a platform application</li><li>Create a platform endpoint</li><li>Publish to the platform endpoint</li><li>Works with Google GCM, Apple APNS, Amazon ADM..</li></ul></li></ul><h3 id="amazon-sns-security" tabindex="-1">Amazon SNS - Security <a class="header-anchor" href="#amazon-sns-security" aria-label="Permalink to “Amazon SNS - Security”">​</a></h3><ul><li>Encryption: <ul><li>In-flight encryption using HTTPS API</li><li>At-rest encryption using KMS keys</li><li>Client-side encryption if the client wants to perform encryption/decryption itself</li></ul></li><li>Access Controls: IAM policies to regulate access to the SNS API</li><li>SNS Access Policies (similar to S3 bucket policies) <ul><li>Useful for cross-account access to SNS topics</li><li>Useful for allowing other services ( S3...) to write to an SNS topic</li></ul></li></ul><h3 id="sns-sqs-fan-out" tabindex="-1">SNS + SQS: Fan Out <a class="header-anchor" href="#sns-sqs-fan-out" aria-label="Permalink to “SNS + SQS: Fan Out”">​</a></h3><ul><li>Push once in SNS, receive in all SQS queues that are subscribers</li><li>Fully decoupled, no data loss</li><li>SQS allows for: data persistence, delayed processing and retries of work</li><li>Ability to add more SQS subscribers over time</li><li>Make sure your SQS queue access policy allows for SNS to write</li></ul><h3 id="application-s3-events-to-multiple-queues" tabindex="-1">Application: S3 Events to multiple queues <a class="header-anchor" href="#application-s3-events-to-multiple-queues" aria-label="Permalink to “Application: S3 Events to multiple queues”">​</a></h3><ul><li>For the same combination of: event type (e.g. object create) and prefix (e.g. images/) you can only have one S3 Event rule</li><li>If you want to send the same S3 event to many SQS queues, use fan-out</li></ul><h3 id="amazon-sns-fifo-topic" tabindex="-1">Amazon SNS - FIFO Topic <a class="header-anchor" href="#amazon-sns-fifo-topic" aria-label="Permalink to “Amazon SNS - FIFO Topic”">​</a></h3><ul><li>FIFO = First In First Out (ordering of messages in the topic)</li><li>Similar features as SQS FIFO: <ul><li>Ordering by Message Group ID (all messages in the same group are ordered)</li><li>Deduplication using a Deduplication ID or Content Based Deduplication</li></ul></li><li>Can only have SQS FIFO queues as subscribers</li><li>Limited throughput (same throughput as SQS FIFO)</li></ul><h3 id="sns-fifo-sqs-fifo-fan-out" tabindex="-1">SNS FIFO + SQS FIFO: Fan Out <a class="header-anchor" href="#sns-fifo-sqs-fifo-fan-out" aria-label="Permalink to “SNS FIFO + SQS FIFO: Fan Out”">​</a></h3><ul><li>In case you need fan out + ordering + deduplication</li></ul><h3 id="sns-message-filtering" tabindex="-1">SNS - Message Filtering <a class="header-anchor" href="#sns-message-filtering" aria-label="Permalink to “SNS - Message Filtering”">​</a></h3><ul><li>JSON policy used to filter messages sent to SNS topic&#39;s subscriptions</li><li>If a subscription doesn&#39;t have a filter policy, it receives every message</li></ul><h2 id="kinesis" tabindex="-1">Kinesis <a class="header-anchor" href="#kinesis" aria-label="Permalink to “Kinesis”">​</a></h2><ul><li>Makes it easy to collect, process, and analyze streaming data in real-time</li><li>Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data...</li><li>Kinesis Data Streams: capture, process, and store data streams</li><li>Kinesis Data Firehose: load data streams into AWS data stores</li><li>Kinesis Data Analytics: analyze data streams with SQL or Apache Flink</li><li>Kinesis Video Streams: capture, process, and store video streams</li></ul><h3 id="kinesis-data-streams" tabindex="-1">Kinesis Data Streams <a class="header-anchor" href="#kinesis-data-streams" aria-label="Permalink to “Kinesis Data Streams”">​</a></h3><ul><li>Billing is per shard provisioned, can have as many shards as you want</li><li>Retention between 1 day (default) to 365 days</li><li>Ability to reprocess (replay) data</li><li>Once data is inserted in Kinesis, it can&#39;t be deleted (immutability)</li><li>Data that shares the same partition goes to the same shard (ordering)</li><li>Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent</li><li>Consumers: <ul><li>Write your own: Kinesis Client Library (KCL), AWS SDK</li><li>Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics,</li></ul></li></ul><h3 id="kinesis-data-streams-security" tabindex="-1">Kinesis Data Streams Security <a class="header-anchor" href="#kinesis-data-streams-security" aria-label="Permalink to “Kinesis Data Streams Security”">​</a></h3><ul><li>Control access / authorization using IAM policies</li><li>Encryption in flight using HTTPS endpoints</li><li>Encryption at rest using KMS</li><li>You can implement encryption/decryption of data on client side (harder)</li><li>VPC Endpoints available for Kinesis to access within VPC</li><li>Monitor API calls using CloudTrail</li></ul><h3 id="kinesis-producers" tabindex="-1">Kinesis Producers <a class="header-anchor" href="#kinesis-producers" aria-label="Permalink to “Kinesis Producers”">​</a></h3><ul><li>Puts data records into data streams</li><li>Data record consists of: <ul><li>Sequence number (unique per partition-key within shard)</li><li>Partition key (must specify while put records into stream)</li><li>Data blob (up to 1 MB)</li></ul></li><li>Producers: <ul><li>AWS SDK: simple producer</li><li>Kinesis Producer Library (KPL): C++, Java, batch, compression, retries</li><li>Kinesis Agent: monitor log files</li></ul></li><li>Write throughput: 1 MB/sec or 1000 records/sec per shard</li><li>PutRecord API</li><li>Use batching with PutRecords API to reduce costs &amp; increase throughput</li></ul><h3 id="kinesis-provisionedthroughputexceeded" tabindex="-1">Kinesis - ProvisionedThroughputExceeded <a class="header-anchor" href="#kinesis-provisionedthroughputexceeded" aria-label="Permalink to “Kinesis - ProvisionedThroughputExceeded”">​</a></h3><ul><li>Solution: <ul><li>Use highly distributed partition key</li><li>Retries with exponential backoff</li><li>Increase shards (scaling)</li></ul></li></ul><h3 id="kinesis-data-streams-consumers" tabindex="-1">Kinesis Data Streams Consumers <a class="header-anchor" href="#kinesis-data-streams-consumers" aria-label="Permalink to “Kinesis Data Streams Consumers”">​</a></h3><ul><li>Get data records from data streams and process them</li><li>AWS Lambda</li><li>Kinesis Data Analytics</li><li>Kinesis Data Firehose</li><li>Custom Consumer (AWS SDK) - Classic or Enhanced Fan-Out</li><li>Kinesis Client Library (KCL): library to simplify reading from data stream</li></ul><h3 id="kinesis-consumers-custom-consumer" tabindex="-1">Kinesis Consumers - Custom Consumer <a class="header-anchor" href="#kinesis-consumers-custom-consumer" aria-label="Permalink to “Kinesis Consumers - Custom Consumer”">​</a></h3><ul><li>Shared (Classic) Fan-out Consumer: 2 MiB/Sec per shard across all consumers</li><li>Enhanced Fan-out Consumer: 2 MiB/Sec per consumer per shard</li></ul><h3 id="kinesis-consumers-types" tabindex="-1">Kinesis Consumers Types <a class="header-anchor" href="#kinesis-consumers-types" aria-label="Permalink to “Kinesis Consumers Types”">​</a></h3><table tabindex="0"><thead><tr><th>Shared (Classic) Fan-out Consumer - pull</th><th>Enhanced Fan-out Consumer - push</th></tr></thead><tbody><tr><td>Low number of consuming applications</td><td>Multiple consuming applications for the same stream</td></tr><tr><td>Read throughput: 2 MB/sec per shard across all consumers</td><td>2 MB/sec per consumer per shard</td></tr><tr><td>Max. 5 GetRecords API calls/sec</td><td>Latency ~70 ms</td></tr><tr><td>Latency ~200 ms</td><td>Higher costs ($$$)</td></tr><tr><td>Minimize cost ($)</td><td>Kinesis pushes data to consumers over HTTP/2 (SubscribeToShard API)</td></tr><tr><td>Consumers poll data from Kinesis using GetRecords API call</td><td>Soft limit of 5 consumer applications (KCL) per data stream (default)</td></tr><tr><td>Returns up to 10 MB (then throttle for 5 seconds) or up to 10000 records</td><td></td></tr></tbody></table><h3 id="kinesis-consumers-aws-lambda" tabindex="-1">Kinesis Consumers - AWS Lambda <a class="header-anchor" href="#kinesis-consumers-aws-lambda" aria-label="Permalink to “Kinesis Consumers - AWS Lambda”">​</a></h3><ul><li>Supports Classic &amp; Enhanced Lambda functions fan-out consumers</li><li>Read records in batches</li><li>Can configure batch size and batch window</li><li>If error occurs, Lambda retries until succeeds or data expired</li><li>Can process up to 10 batches per shard simultaneously</li></ul><h3 id="kinesis-client-library-kcl" tabindex="-1">Kinesis Client Library (KCL) <a class="header-anchor" href="#kinesis-client-library-kcl" aria-label="Permalink to “Kinesis Client Library (KCL)”">​</a></h3><ul><li>A Java library that helps read record from a Kinesis Data Stream with distributed applications sharing the read workload</li><li>Each shard is to be read by only one KCL instance <ul><li>4 shards = max. 4 KCL instances</li><li>6 shards = max. 6 KCL instances</li></ul></li><li>Progress is checkpointed into DynamoDB (needs IAM access)</li><li>Track other workers and share the work amongst shards using DynamoDB</li><li>KCL can run on EC2, Elastic Beanstalk, and on-premises</li><li>Records are read in order at the shard level</li><li>Versions: <ul><li>KCL 1.x (supports shared consumer)</li><li>KCL 2.x (supports shared &amp; enhanced fan-out consumer)</li></ul></li></ul><h3 id="kinesis-operation-shard-splitting" tabindex="-1">Kinesis Operation - Shard Splitting <a class="header-anchor" href="#kinesis-operation-shard-splitting" aria-label="Permalink to “Kinesis Operation - Shard Splitting”">​</a></h3><ul><li>Used to increase the Stream capacity (1 MB/s data in per shard)</li><li>Used to divide a &quot;hot shard&quot;</li><li>The old shard is closed and will be deleted once the data is expired</li><li>No automatic scaling (manually increase/decrease capacity)</li><li>Can&#39;t split into more than two shards in a single operation</li></ul><h3 id="kinesis-operation-merging-shards" tabindex="-1">Kinesis Operation - Merging Shards <a class="header-anchor" href="#kinesis-operation-merging-shards" aria-label="Permalink to “Kinesis Operation - Merging Shards”">​</a></h3><ul><li>Decrease the Stream capacity and save costs</li><li>Can be used to group two shards with low traffic (cold shards)</li><li>Old shards are closed and will be deleted once the data is expired</li><li>Can&#39;t merge more than two shards in a single operation</li></ul><h3 id="kinesis-data-firehose" tabindex="-1">Kinesis Data Firehose <a class="header-anchor" href="#kinesis-data-firehose" aria-label="Permalink to “Kinesis Data Firehose”">​</a></h3><ul><li>Fully Managed Service, no administration, automatic scaling, serverless <ul><li>AWS: Redshift / Amazon S3 / ElasticSearch</li><li>3rd party partner: Splunk / MongoDB / DataDog / NewRelic / ..</li><li>Custom: send to any HTTP endpoint</li></ul></li><li>Pay for data going through Firehose</li><li>Near Real Time <ul><li>60 seconds latency minimum for non full batches</li><li>Or minimum 32 MB of data at a time</li></ul></li><li>Supports many data formats, conversions, transformations, compression</li><li>Supports custom data transformations using AWS Lambda</li><li>Can send failed or all data to a backup S3 bucket</li></ul><h3 id="kinesis-data-streams-vs-firehose" tabindex="-1">Kinesis Data Streams vs Firehose <a class="header-anchor" href="#kinesis-data-streams-vs-firehose" aria-label="Permalink to “Kinesis Data Streams vs Firehose”">​</a></h3><table tabindex="0"><thead><tr><th>Kinesis Data Streams</th><th>Kinesis Data Firehose</th></tr></thead><tbody><tr><td>Streaming service for ingest at scale</td><td>Load streaming data into S3 / Redshift / ES / 3rd party / custom HTTP</td></tr><tr><td>Write custom code (producer / consumer)</td><td>Fully managed</td></tr><tr><td>Real-time (~200 ms)</td><td>Near real-time (buffer time min. 60 sec)</td></tr><tr><td>Manage scaling (shard splitting / merging)</td><td>Automatic scaling</td></tr><tr><td>Data storage for 1 to 365 days</td><td>No data storage</td></tr><tr><td>Supports replay capability</td><td>Doesn&#39;t support replay capability</td></tr></tbody></table><h3 id="kinesis-data-analytics-sql-application" tabindex="-1">Kinesis Data Analytics (SQL application) <a class="header-anchor" href="#kinesis-data-analytics-sql-application" aria-label="Permalink to “Kinesis Data Analytics (SQL application)”">​</a></h3><ul><li>Perform real-time analytics on Kinesis Streams using SQL</li><li>Fully managed, no servers to provision</li><li>Automatic scaling</li><li>Real-time analytics</li><li>Pay for actual consumption rate</li><li>Can create streams out of the real-time queries</li><li>Use cases: <ul><li>Time-series analytics</li><li>Real-time dashboards</li><li>Real-time metrics</li></ul></li></ul><h3 id="ordering-data-into-kinesis" tabindex="-1">Ordering data into Kinesis <a class="header-anchor" href="#ordering-data-into-kinesis" aria-label="Permalink to “Ordering data into Kinesis”">​</a></h3><ul><li>Imagine you have <code>100</code> trucks (<code>truck_1</code>, <code>truck_2</code>, ... <code>truck_100</code>) on the road sending their GPS positions regularly into AWS</li><li>You want to consume the data in order for each truck, so that you can track their movement accurately</li><li>How should you send that data into Kinesis?</li><li>Answer: send using a &quot;Partition Key&quot; value of the &quot;truck_id&quot;</li><li>The same key will always go to the same shard</li></ul><h3 id="ordering-data-into-sqs" tabindex="-1">Ordering data into SQS <a class="header-anchor" href="#ordering-data-into-sqs" aria-label="Permalink to “Ordering data into SQS”">​</a></h3><ul><li>For SQS standard, there is no ordering</li><li>For SQS FIFO, if you don&#39;t use a Group ID, messages are consumed in the order they are sent, with only one consumer</li><li>You want to scale the number of consumers, but you want messages to be &quot;grouped&quot; when they are related to each other</li><li>Then you use a Group ID (similar to Partition Key in Kinesis)</li></ul><h3 id="kinesis-vs-sqs-ordering" tabindex="-1">Kinesis vs SQS ordering <a class="header-anchor" href="#kinesis-vs-sqs-ordering" aria-label="Permalink to “Kinesis vs SQS ordering”">​</a></h3><ul><li>Let&#39;s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO</li><li>Kinesis Data Streams: <ul><li>On average you&#39;ll have 20 trucks per shard</li><li>Trucks will have their data ordered within each shard</li><li>The maximum amount of consumers in parallel we can have is 5</li><li>Can receive up to 5 MB/s of data</li></ul></li><li>SQS FIFO <ul><li>You only have one SQS FIFO queue</li><li>You will have 100 Group ID</li><li>You can have up to 100 Consumers (due to the 100 Group ID)</li><li>You have up to 300 messages per second (or 3000 if using batching)</li></ul></li></ul><h3 id="sqs-vs-sns-vs-kinesis" tabindex="-1">SQS vs SNS vs Kinesis <a class="header-anchor" href="#sqs-vs-sns-vs-kinesis" aria-label="Permalink to “SQS vs SNS vs Kinesis”">​</a></h3><table tabindex="0"><thead><tr><th>SQS</th><th>SNS</th><th>Kinesis</th></tr></thead><tbody><tr><td>Consumer &quot;pull data&quot;</td><td>Push data to many subscribers</td><td>Standard: pull data: 2 MB per shard</td></tr><tr><td>Data is deleted after being consumed</td><td>Up to 12,500,000 subscribers</td><td>Enhanced-fan out: push data: 2 MB per shard per consumer</td></tr><tr><td>Can have as many workers (consumers) as we want</td><td>Data is not persisted (lost if not delivered)</td><td>Possibility to replay data</td></tr><tr><td>No need to provision throughput</td><td>Pub/Sub</td><td>Meant for real-time big data, analytics and ETL</td></tr><tr><td>Ordering guarantees only on FIFO queues</td><td>Up to 100,000 topics</td><td>Ordering at the shard level</td></tr><tr><td>Individual message delay capability</td><td>No need to provision throughput</td><td>Data expires after X days</td></tr><tr><td></td><td>Integrates with SQS for fan-out architecture pattern</td><td>Must provision throughput</td></tr><tr><td></td><td>FIFO capability for SQS FIFO</td><td></td></tr></tbody></table>`,90)]])}var s=r(a,[[`render`,o]]);export{i as __pageData,s as default};